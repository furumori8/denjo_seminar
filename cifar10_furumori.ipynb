{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20L59U97Bf6a",
        "outputId": "122d2742-6938-4201-c85b-382ad3b8453d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "# 必要なライブラリのインストール\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1LIYWxPgCeU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 残差ブロックの定義\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, dropout_prob=0.3):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # Dropoutを追加\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample  # スキップ接続用のダウンサンプル層\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x  # スキップ接続のための入力\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)  # Dropoutを適用\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)  # スキップ接続の次元を揃える\n",
        "\n",
        "        out += identity\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNetの定義\n",
        "class CIFAR10ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10, dropout_prob=0.1):\n",
        "        super(CIFAR10ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # 残差ブロックのレイヤー\n",
        "        self.layer1 = self._make_layer(32, 64, 2, stride=1, dropout_prob=dropout_prob/3)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2, dropout_prob=dropout_prob/2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2, dropout_prob=dropout_prob/1.5)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2, dropout_prob=dropout_prob)\n",
        "        # self.layer5 = self._make_layer(512, 1024, 1, stride=2, dropout_prob=dropout_prob)\n",
        "\n",
        "        self.maxpool = nn.AdaptiveAvgPool2d((1, 1))  # 全体の平均をとるように変更\n",
        "        self.fc = nn.Linear(128 * 4, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # FC層の前にDropoutを追加\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride=1, dropout_prob=0.1):\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample, dropout_prob))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels, dropout_prob=dropout_prob))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        # x = self.layer5(x)\n",
        "\n",
        "        x = self.maxpool(x)\n",
        "        x = torch.flatten(x, 1)  # 平坦化\n",
        "        x = self.dropout(x)  # Dropoutを適用\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.1, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            # 正解ラベルをスムージング\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO41v7hxi0ek"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "# 確率勾配降下法で最適化する手法やパラメータを選択する\n",
        "# ToDo: SGDとAdamの違いについて調査する\n",
        "#optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "#adamに変更"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwbZdcBbcBuv"
      },
      "outputs": [],
      "source": [
        "# テンソルを送るデバイスを選択する\n",
        "# Neural Networkを学習する場合、並列計算を行うため基本的にはGPUを選択\n",
        "def get_device(gpu_id=-1):\n",
        "    if gpu_id >= 0 and torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\", gpu_id)\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "#optimizer = optimizer.to(device)\n",
        "#loss_fn = F.nll_loss().to(device)\n",
        "#loss_fn = F.nll_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjn0BCF7c_gu",
        "outputId": "ac84a12e-83c3-46eb-bc58-b580218f6b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg81c4PLi2Zp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bdfb880-d76a-4ec9-f956-db2ab654936e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-6b4144ded2a6>:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.cuda.amp import GradScaler\n",
        "from torch.amp import autocast\n",
        "# import time\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "def train(Epoch, Net, Train_dataloader, Val_daraloader, optimizer_):\n",
        "  for epoch in range(Epoch):\n",
        "    Net.train()\n",
        "    running_loss = 0.0  # ここで初期化\n",
        "    acc = 0.0           # ここで初期化\n",
        "    # start = time.time()\n",
        "    for i, data in enumerate(Train_dataloader):\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      with autocast(device_type='cuda'):\n",
        "          outputs = Net(inputs)\n",
        "          loss = loss_fn(outputs, labels)\n",
        "\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "      # 損失と精度の計算\n",
        "      running_loss += loss.item()\n",
        "      preds = torch.argmax(outputs, dim=1)\n",
        "      acc += (preds == labels).sum().item() / batch_size_\n",
        "\n",
        "\n",
        "      if i % 500 == 499:\n",
        "          print(f\"epoch: {epoch}, iter: {i + 1}, train_loss: {running_loss / 500}, train_acc: {acc/500 * 100}\")\n",
        "          running_loss = 0.0\n",
        "          acc = 0\n",
        "    # end = time.time()\n",
        "    # print(f\"epoch: {epoch}, time: {end - start}\")\n",
        "    scheduler.step()\n",
        "    # if(epoch > 10):\n",
        "      # val(Net, Val_daraloader)\n",
        "\n",
        "  print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzqEtHzEC3Zt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# 検証モード\n",
        "# ToDo: 訓練モードと比較して振る舞いはどう変化する？\n",
        "def val(Net, val_dataloader):\n",
        "    Net.eval()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    cnt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in val_dataloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with autocast(device_type='cuda'):  # ここも修正\n",
        "                outputs = Net(inputs)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            total_acc += (preds == labels).sum().item() / batch_size_\n",
        "            cnt += 1\n",
        "\n",
        "    print(f\"Validation Loss: {total_loss / cnt}, Validation Accuracy: {total_acc / cnt * 100}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJfn_9niazEQ",
        "outputId": "92e46519-072c-4c03-9054-d3f066068c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# net = CIFAR10ResNet(num_classes=10)\n",
        "\n",
        "device = get_device(gpu_id=0)\n",
        "print(device)\n",
        "\n",
        "\n",
        "# softmax = nn.Softmax(dim=1) #確率の正規化（全てのクラスの確率の合計=1）\n",
        "\n",
        "# def net_init(tmp_net):\n",
        "#   optimizer = optim.Adam(tmp_net.parameters(), lr=0.001)\n",
        "#   tmp_net = tmp_net.to(device) #モデルを実際に転送\n",
        "  # 損失関数\n",
        "  # この損失を微分し、勾配降下法によって各パラメータを更新する\n",
        "  # ToDo: その他の損失関数について調べる。CrossEntropyLoss以外にも有効な関数があるかも...!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu_ESQkqf5ui",
        "outputId": "299d6d00-7f19-4431-801d-a05078757b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/tmp/cifar/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 13.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/tmp/cifar/cifar-10-python.tar.gz to /root/tmp/cifar\n",
            "Files already downloaded and verified\n",
            "Fold 1\n",
            "epoch: 0, iter: 500, train_loss: 1.787160111427307, train_acc: 40.884375\n",
            "epoch: 1, iter: 500, train_loss: 1.4478786239624024, train_acc: 58.0125\n",
            "epoch: 2, iter: 500, train_loss: 1.305425691843033, train_acc: 65.19375\n",
            "epoch: 3, iter: 500, train_loss: 1.19325439286232, train_acc: 70.865625\n",
            "epoch: 4, iter: 500, train_loss: 1.1240886706113815, train_acc: 74.121875\n",
            "epoch: 5, iter: 500, train_loss: 1.0577392665147782, train_acc: 77.075\n",
            "epoch: 6, iter: 500, train_loss: 1.0158108251094817, train_acc: 79.06875\n",
            "epoch: 7, iter: 500, train_loss: 0.9748799475431442, train_acc: 80.94687499999999\n",
            "epoch: 8, iter: 500, train_loss: 0.9552147091627121, train_acc: 81.7125\n",
            "epoch: 9, iter: 500, train_loss: 0.9243268812894821, train_acc: 83.321875\n",
            "epoch: 10, iter: 500, train_loss: 0.8968439078330994, train_acc: 84.503125\n",
            "epoch: 11, iter: 500, train_loss: 0.8761167448759078, train_acc: 85.39375\n",
            "epoch: 12, iter: 500, train_loss: 0.848228317975998, train_acc: 86.471875\n",
            "epoch: 13, iter: 500, train_loss: 0.8298590075969696, train_acc: 87.490625\n",
            "epoch: 14, iter: 500, train_loss: 0.8142915103435516, train_acc: 88.253125\n",
            "epoch: 15, iter: 500, train_loss: 0.8036707227230072, train_acc: 88.721875\n",
            "epoch: 16, iter: 500, train_loss: 0.7824069024324417, train_acc: 89.5625\n",
            "epoch: 17, iter: 500, train_loss: 0.7677294490337372, train_acc: 90.2375\n",
            "epoch: 18, iter: 500, train_loss: 0.7568805947303772, train_acc: 90.6875\n",
            "epoch: 19, iter: 500, train_loss: 0.7448211945295334, train_acc: 91.459375\n",
            "epoch: 20, iter: 500, train_loss: 0.7303503719568253, train_acc: 91.93124999999999\n",
            "epoch: 21, iter: 500, train_loss: 0.717168163061142, train_acc: 92.565625\n",
            "epoch: 22, iter: 500, train_loss: 0.7039673488140106, train_acc: 93.084375\n",
            "epoch: 23, iter: 500, train_loss: 0.6984746236801147, train_acc: 93.540625\n",
            "epoch: 24, iter: 500, train_loss: 0.6915127835273742, train_acc: 93.778125\n",
            "epoch: 25, iter: 500, train_loss: 0.6725330798625946, train_acc: 94.7375\n",
            "epoch: 26, iter: 500, train_loss: 0.6688516572713852, train_acc: 94.684375\n",
            "epoch: 27, iter: 500, train_loss: 0.6656159770488739, train_acc: 94.984375\n",
            "epoch: 28, iter: 500, train_loss: 0.6578572295904159, train_acc: 95.23125\n",
            "epoch: 29, iter: 500, train_loss: 0.6474949907064438, train_acc: 95.79062499999999\n",
            "epoch: 30, iter: 500, train_loss: 0.6424964570999145, train_acc: 96.103125\n",
            "epoch: 31, iter: 500, train_loss: 0.6386864545345307, train_acc: 96.125\n",
            "epoch: 32, iter: 500, train_loss: 0.6340153532028199, train_acc: 96.37187499999999\n",
            "epoch: 33, iter: 500, train_loss: 0.6299244101047516, train_acc: 96.61874999999999\n",
            "epoch: 34, iter: 500, train_loss: 0.6281570172309876, train_acc: 96.6375\n",
            "epoch: 35, iter: 500, train_loss: 0.6270636154413223, train_acc: 96.7125\n",
            "epoch: 36, iter: 500, train_loss: 0.6238901426792145, train_acc: 96.83125\n",
            "epoch: 37, iter: 500, train_loss: 0.621577127456665, train_acc: 96.928125\n",
            "epoch: 38, iter: 500, train_loss: 0.6223394935131072, train_acc: 96.85625\n",
            "epoch: 39, iter: 500, train_loss: 0.6199119880199432, train_acc: 97.078125\n",
            "Finished Training\n",
            "Validation Loss: 0.711597447942017, Validation Accuracy: 92.7547770700637\n",
            "Training and validation for fold 1 completed.\n",
            "\n",
            "Fold 2\n",
            "epoch: 0, iter: 500, train_loss: 1.8073603830337523, train_acc: 38.93125\n",
            "epoch: 1, iter: 500, train_loss: 1.4454413676261901, train_acc: 58.00625\n",
            "epoch: 2, iter: 500, train_loss: 1.287992069005966, train_acc: 66.059375\n",
            "epoch: 3, iter: 500, train_loss: 1.176982340335846, train_acc: 71.484375\n",
            "epoch: 4, iter: 500, train_loss: 1.0978145736455918, train_acc: 75.29375\n",
            "epoch: 5, iter: 500, train_loss: 1.045405169725418, train_acc: 77.834375\n",
            "epoch: 6, iter: 500, train_loss: 1.008609138727188, train_acc: 79.284375\n",
            "epoch: 7, iter: 500, train_loss: 0.9690936132669449, train_acc: 81.328125\n",
            "epoch: 8, iter: 500, train_loss: 0.933092411160469, train_acc: 82.715625\n",
            "epoch: 9, iter: 500, train_loss: 0.9110080391168595, train_acc: 83.825\n",
            "epoch: 10, iter: 500, train_loss: 0.8890075135231018, train_acc: 84.834375\n",
            "epoch: 11, iter: 500, train_loss: 0.8664621343612671, train_acc: 85.85000000000001\n",
            "epoch: 12, iter: 500, train_loss: 0.8479745939970017, train_acc: 86.63125000000001\n",
            "epoch: 13, iter: 500, train_loss: 0.8252105633020401, train_acc: 87.790625\n",
            "epoch: 14, iter: 500, train_loss: 0.8095390077829361, train_acc: 88.5\n",
            "epoch: 15, iter: 500, train_loss: 0.7916682406663894, train_acc: 89.15625\n",
            "epoch: 16, iter: 500, train_loss: 0.7768568584918976, train_acc: 89.84375\n",
            "epoch: 17, iter: 500, train_loss: 0.7642532544136047, train_acc: 90.434375\n",
            "epoch: 18, iter: 500, train_loss: 0.7465704497098923, train_acc: 91.25937499999999\n",
            "epoch: 19, iter: 500, train_loss: 0.7413393139839173, train_acc: 91.3625\n",
            "epoch: 20, iter: 500, train_loss: 0.7233477147817612, train_acc: 92.253125\n",
            "epoch: 21, iter: 500, train_loss: 0.713303559422493, train_acc: 92.559375\n",
            "epoch: 22, iter: 500, train_loss: 0.7041369354724885, train_acc: 92.99374999999999\n",
            "epoch: 23, iter: 500, train_loss: 0.6944463241100312, train_acc: 93.6875\n",
            "epoch: 24, iter: 500, train_loss: 0.6793822387456894, train_acc: 94.296875\n",
            "epoch: 25, iter: 500, train_loss: 0.6747047724723816, train_acc: 94.540625\n",
            "epoch: 26, iter: 500, train_loss: 0.6647191694974899, train_acc: 94.9125\n",
            "epoch: 27, iter: 500, train_loss: 0.6605667674541473, train_acc: 95.09062499999999\n",
            "epoch: 28, iter: 500, train_loss: 0.6538386787176133, train_acc: 95.465625\n",
            "epoch: 29, iter: 500, train_loss: 0.6425562368631363, train_acc: 95.95\n",
            "epoch: 30, iter: 500, train_loss: 0.6423547914028168, train_acc: 95.909375\n",
            "epoch: 31, iter: 500, train_loss: 0.6374675152301789, train_acc: 96.103125\n",
            "epoch: 32, iter: 500, train_loss: 0.6289706157445908, train_acc: 96.51875\n",
            "epoch: 33, iter: 500, train_loss: 0.6275430707931519, train_acc: 96.584375\n",
            "epoch: 34, iter: 500, train_loss: 0.6230115158557892, train_acc: 96.921875\n",
            "epoch: 35, iter: 500, train_loss: 0.6206924071311951, train_acc: 96.9375\n",
            "epoch: 36, iter: 500, train_loss: 0.6196215863227844, train_acc: 96.90625\n",
            "epoch: 37, iter: 500, train_loss: 0.6154097372293472, train_acc: 97.334375\n",
            "epoch: 38, iter: 500, train_loss: 0.6171699892282486, train_acc: 97.046875\n",
            "epoch: 39, iter: 500, train_loss: 0.6140965986251831, train_acc: 97.284375\n",
            "Finished Training\n",
            "Validation Loss: 0.711138491797599, Validation Accuracy: 92.81449044585987\n",
            "Training and validation for fold 2 completed.\n",
            "\n",
            "Fold 3\n",
            "epoch: 0, iter: 500, train_loss: 1.8069384593963622, train_acc: 39.484375\n",
            "epoch: 1, iter: 500, train_loss: 1.442678507566452, train_acc: 58.40625000000001\n",
            "epoch: 2, iter: 500, train_loss: 1.3012817554473877, train_acc: 65.23125\n",
            "epoch: 3, iter: 500, train_loss: 1.2056293866634369, train_acc: 70.065625\n",
            "epoch: 4, iter: 500, train_loss: 1.1244549106359483, train_acc: 74.10624999999999\n",
            "epoch: 5, iter: 500, train_loss: 1.0599991608858108, train_acc: 77.096875\n",
            "epoch: 6, iter: 500, train_loss: 1.0178031009435653, train_acc: 79.203125\n",
            "epoch: 7, iter: 500, train_loss: 0.9770184128284455, train_acc: 80.93125\n",
            "epoch: 8, iter: 500, train_loss: 0.9447789471149445, train_acc: 82.28125\n",
            "epoch: 9, iter: 500, train_loss: 0.9178578753471375, train_acc: 83.53437500000001\n",
            "epoch: 10, iter: 500, train_loss: 0.8925854811668396, train_acc: 84.690625\n",
            "epoch: 11, iter: 500, train_loss: 0.8699952901601792, train_acc: 85.82499999999999\n",
            "epoch: 12, iter: 500, train_loss: 0.8492541916370392, train_acc: 86.75\n",
            "epoch: 13, iter: 500, train_loss: 0.8287412468194961, train_acc: 87.546875\n",
            "epoch: 14, iter: 500, train_loss: 0.8105220338106155, train_acc: 88.26875\n",
            "epoch: 15, iter: 500, train_loss: 0.7927479389905929, train_acc: 89.221875\n",
            "epoch: 16, iter: 500, train_loss: 0.7775249884128571, train_acc: 89.809375\n",
            "epoch: 17, iter: 500, train_loss: 0.7659283225536346, train_acc: 90.278125\n",
            "epoch: 18, iter: 500, train_loss: 0.7473594771623612, train_acc: 91.315625\n",
            "epoch: 19, iter: 500, train_loss: 0.7408362329006195, train_acc: 91.4625\n",
            "epoch: 20, iter: 500, train_loss: 0.7241676436662674, train_acc: 92.278125\n",
            "epoch: 21, iter: 500, train_loss: 0.7154059315919876, train_acc: 92.5875\n",
            "epoch: 22, iter: 500, train_loss: 0.7033381114006042, train_acc: 93.16875\n",
            "epoch: 23, iter: 500, train_loss: 0.6961281754970551, train_acc: 93.525\n",
            "epoch: 24, iter: 500, train_loss: 0.685051112651825, train_acc: 93.9625\n",
            "epoch: 25, iter: 500, train_loss: 0.6768715121746063, train_acc: 94.36875\n",
            "epoch: 26, iter: 500, train_loss: 0.668180891752243, train_acc: 94.846875\n",
            "epoch: 27, iter: 500, train_loss: 0.6582786871194839, train_acc: 95.23125\n",
            "epoch: 28, iter: 500, train_loss: 0.6542398942708969, train_acc: 95.359375\n",
            "epoch: 29, iter: 500, train_loss: 0.6463188021183014, train_acc: 95.75625\n",
            "epoch: 30, iter: 500, train_loss: 0.6387653180360794, train_acc: 96.109375\n",
            "epoch: 31, iter: 500, train_loss: 0.6351184763908386, train_acc: 96.24062500000001\n",
            "epoch: 32, iter: 500, train_loss: 0.6310020557641983, train_acc: 96.5\n",
            "epoch: 33, iter: 500, train_loss: 0.6262695409059524, train_acc: 96.746875\n",
            "epoch: 34, iter: 500, train_loss: 0.6276414943933487, train_acc: 96.65312499999999\n",
            "epoch: 35, iter: 500, train_loss: 0.6178716282844543, train_acc: 97.13437499999999\n",
            "epoch: 36, iter: 500, train_loss: 0.6200330247879028, train_acc: 96.93437499999999\n",
            "epoch: 37, iter: 500, train_loss: 0.6195186415910721, train_acc: 97.02499999999999\n",
            "epoch: 38, iter: 500, train_loss: 0.6156546270847321, train_acc: 97.15625\n",
            "epoch: 39, iter: 500, train_loss: 0.615540302991867, train_acc: 97.15312499999999\n",
            "Finished Training\n",
            "Validation Loss: 0.7009548385431812, Validation Accuracy: 93.24243630573248\n",
            "Training and validation for fold 3 completed.\n",
            "\n",
            "Fold 4\n",
            "epoch: 0, iter: 500, train_loss: 1.838545922756195, train_acc: 37.65625\n",
            "epoch: 1, iter: 500, train_loss: 1.4513074324131012, train_acc: 57.818749999999994\n",
            "epoch: 2, iter: 500, train_loss: 1.3038528063297272, train_acc: 65.15625\n",
            "epoch: 3, iter: 500, train_loss: 1.1955201852321624, train_acc: 70.471875\n",
            "epoch: 4, iter: 500, train_loss: 1.1170835129022598, train_acc: 74.23125\n",
            "epoch: 5, iter: 500, train_loss: 1.055320391178131, train_acc: 77.190625\n",
            "epoch: 6, iter: 500, train_loss: 1.019768181681633, train_acc: 78.925\n",
            "epoch: 7, iter: 500, train_loss: 0.9778628226518631, train_acc: 80.78125\n",
            "epoch: 8, iter: 500, train_loss: 0.9459276283979416, train_acc: 82.434375\n",
            "epoch: 9, iter: 500, train_loss: 0.9179329040050507, train_acc: 83.6375\n",
            "epoch: 10, iter: 500, train_loss: 0.8942430912256241, train_acc: 84.51875\n",
            "epoch: 11, iter: 500, train_loss: 0.8705458263158798, train_acc: 85.68124999999999\n",
            "epoch: 12, iter: 500, train_loss: 0.8489015573263168, train_acc: 86.79375\n",
            "epoch: 13, iter: 500, train_loss: 0.8301470338106155, train_acc: 87.465625\n",
            "epoch: 14, iter: 500, train_loss: 0.8124038053750992, train_acc: 88.36874999999999\n",
            "epoch: 15, iter: 500, train_loss: 0.7971636142730713, train_acc: 89.08437500000001\n",
            "epoch: 16, iter: 500, train_loss: 0.7792344559431076, train_acc: 89.778125\n",
            "epoch: 17, iter: 500, train_loss: 0.7669324071407319, train_acc: 90.40312499999999\n",
            "epoch: 18, iter: 500, train_loss: 0.754703757762909, train_acc: 90.809375\n",
            "epoch: 19, iter: 500, train_loss: 0.7354657436609269, train_acc: 91.678125\n",
            "epoch: 20, iter: 500, train_loss: 0.7259015308618546, train_acc: 92.078125\n",
            "epoch: 21, iter: 500, train_loss: 0.7126160291433334, train_acc: 92.74687499999999\n",
            "epoch: 22, iter: 500, train_loss: 0.70503737449646, train_acc: 93.0125\n",
            "epoch: 23, iter: 500, train_loss: 0.695174200296402, train_acc: 93.5\n",
            "epoch: 24, iter: 500, train_loss: 0.6862021321058274, train_acc: 93.925\n",
            "epoch: 25, iter: 500, train_loss: 0.6769652183055878, train_acc: 94.1875\n",
            "epoch: 26, iter: 500, train_loss: 0.6683638619184494, train_acc: 94.75\n",
            "epoch: 27, iter: 500, train_loss: 0.6586434482336044, train_acc: 95.15\n",
            "epoch: 28, iter: 500, train_loss: 0.6538847326040268, train_acc: 95.45937500000001\n",
            "epoch: 29, iter: 500, train_loss: 0.6482312443256378, train_acc: 95.753125\n",
            "epoch: 30, iter: 500, train_loss: 0.6416072479486465, train_acc: 95.984375\n",
            "epoch: 31, iter: 500, train_loss: 0.6359001049995422, train_acc: 96.384375\n",
            "epoch: 32, iter: 500, train_loss: 0.6358775613307953, train_acc: 96.190625\n",
            "epoch: 33, iter: 500, train_loss: 0.6286476179361343, train_acc: 96.59375\n",
            "epoch: 34, iter: 500, train_loss: 0.6237497086524963, train_acc: 96.79375\n",
            "epoch: 35, iter: 500, train_loss: 0.6255887001752853, train_acc: 96.684375\n",
            "epoch: 36, iter: 500, train_loss: 0.6235758168697357, train_acc: 96.85625\n",
            "epoch: 37, iter: 500, train_loss: 0.6184114226102829, train_acc: 97.125\n",
            "epoch: 38, iter: 500, train_loss: 0.6170666615962982, train_acc: 97.0625\n",
            "epoch: 39, iter: 500, train_loss: 0.618937395811081, train_acc: 96.96875\n",
            "Finished Training\n",
            "Validation Loss: 0.7051967234368537, Validation Accuracy: 92.76472929936305\n",
            "Training and validation for fold 4 completed.\n",
            "\n",
            "Fold 5\n",
            "epoch: 0, iter: 500, train_loss: 1.8209834568500518, train_acc: 38.596875000000004\n",
            "epoch: 1, iter: 500, train_loss: 1.454419094324112, train_acc: 57.746874999999996\n",
            "epoch: 2, iter: 500, train_loss: 1.3165883929729463, train_acc: 64.590625\n",
            "epoch: 3, iter: 500, train_loss: 1.2088760105371474, train_acc: 69.759375\n",
            "epoch: 4, iter: 500, train_loss: 1.1308157227039337, train_acc: 73.878125\n",
            "epoch: 5, iter: 500, train_loss: 1.0658446611166001, train_acc: 76.88125\n",
            "epoch: 6, iter: 500, train_loss: 1.0132181802988052, train_acc: 78.94375\n",
            "epoch: 7, iter: 500, train_loss: 0.9853329572677613, train_acc: 80.396875\n",
            "epoch: 8, iter: 500, train_loss: 0.9526884547472, train_acc: 81.90625\n",
            "epoch: 9, iter: 500, train_loss: 0.9247302503585816, train_acc: 83.096875\n",
            "epoch: 10, iter: 500, train_loss: 0.8953540128469467, train_acc: 84.471875\n",
            "epoch: 11, iter: 500, train_loss: 0.8711778107881546, train_acc: 85.390625\n",
            "epoch: 12, iter: 500, train_loss: 0.8530450378656387, train_acc: 86.371875\n",
            "epoch: 13, iter: 500, train_loss: 0.8319874639511109, train_acc: 87.46875\n",
            "epoch: 14, iter: 500, train_loss: 0.8162806824445724, train_acc: 88.2875\n",
            "epoch: 15, iter: 500, train_loss: 0.7983613814115524, train_acc: 88.853125\n",
            "epoch: 16, iter: 500, train_loss: 0.7807346397638321, train_acc: 89.75625000000001\n",
            "epoch: 17, iter: 500, train_loss: 0.7674734500646592, train_acc: 90.271875\n",
            "epoch: 18, iter: 500, train_loss: 0.7534804344177246, train_acc: 91.021875\n",
            "epoch: 19, iter: 500, train_loss: 0.7405041794776916, train_acc: 91.60312499999999\n",
            "epoch: 20, iter: 500, train_loss: 0.7289630533456802, train_acc: 92.096875\n",
            "epoch: 21, iter: 500, train_loss: 0.7183686162233353, train_acc: 92.459375\n",
            "epoch: 22, iter: 500, train_loss: 0.7086437404155731, train_acc: 92.928125\n",
            "epoch: 23, iter: 500, train_loss: 0.6932218784093857, train_acc: 93.71875\n",
            "epoch: 24, iter: 500, train_loss: 0.6868528560400009, train_acc: 93.99374999999999\n",
            "epoch: 25, iter: 500, train_loss: 0.6772161182165146, train_acc: 94.29375\n",
            "epoch: 26, iter: 500, train_loss: 0.6710002086162568, train_acc: 94.715625\n",
            "epoch: 27, iter: 500, train_loss: 0.6631246068477631, train_acc: 95.0375\n",
            "epoch: 28, iter: 500, train_loss: 0.6558386442661286, train_acc: 95.259375\n",
            "epoch: 29, iter: 500, train_loss: 0.6500765142440796, train_acc: 95.55312500000001\n",
            "epoch: 30, iter: 500, train_loss: 0.6436995455026626, train_acc: 95.8875\n",
            "epoch: 31, iter: 500, train_loss: 0.6373283125162125, train_acc: 96.184375\n",
            "epoch: 32, iter: 500, train_loss: 0.6328112179040909, train_acc: 96.33125\n",
            "epoch: 33, iter: 500, train_loss: 0.6317330013513565, train_acc: 96.48125\n",
            "epoch: 34, iter: 500, train_loss: 0.6286649234294891, train_acc: 96.56875000000001\n",
            "epoch: 35, iter: 500, train_loss: 0.6249000179767609, train_acc: 96.83125\n",
            "epoch: 36, iter: 500, train_loss: 0.6234482051134109, train_acc: 96.85625\n",
            "epoch: 37, iter: 500, train_loss: 0.6184196716547012, train_acc: 97.046875\n",
            "epoch: 38, iter: 500, train_loss: 0.6215222918987274, train_acc: 96.91875\n",
            "epoch: 39, iter: 500, train_loss: 0.6192412632703781, train_acc: 97.03125\n",
            "Finished Training\n",
            "Validation Loss: 0.7017911737132224, Validation Accuracy: 92.9140127388535\n",
            "Training and validation for fold 5 completed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# 前処理の定義\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),                   # ランダムに水平反転\n",
        "    transforms.RandomCrop(32, padding=4),                # 4ピクセルのパディングを加えた後、32x32のランダムクロップ\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),  # 正規化\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.05, 0.2), ratio=(0.3, 3.3), value='random')  # Random Erasingの追加\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n",
        "])\n",
        "\n",
        "# CIFAR-10の訓練データの読み込み（前処理は訓練用）\n",
        "full_train_data = CIFAR10(root='~/tmp/cifar', train=True, download=True, transform=train_transform)\n",
        "\n",
        "# KFoldの設定\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=44)\n",
        "\n",
        "# テストデータの読み込み（前処理はテスト用）\n",
        "test_data = CIFAR10(root='~/tmp/cifar', train=False, download=True, transform=test_transform)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "\n",
        "models = []\n",
        "\n",
        "# 各Foldでの訓練と検証\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(full_train_data)):\n",
        "    print(f'Fold {fold + 1}')\n",
        "\n",
        "    # 訓練用と検証用のサブセットを作成\n",
        "    train_subset = Subset(full_train_data, train_indices)\n",
        "\n",
        "    # 検証用のデータセットはテスト時の前処理が必要なので、別途ロード\n",
        "    # 検証用データにはテスト用のtransformを適用\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n",
        "    ])\n",
        "    full_val_data = CIFAR10(root='~/tmp/cifar', train=True, download=False, transform=val_transform)\n",
        "    val_subset = Subset(full_val_data, val_indices)\n",
        "\n",
        "    # データローダーの定義\n",
        "    batch_size_ = 64\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size_, shuffle=True)\n",
        "    validation_loader = DataLoader(val_subset, batch_size=batch_size_, shuffle=False)\n",
        "\n",
        "    tmp_net = CIFAR10ResNet(num_classes=10, dropout_prob=0.1)\n",
        "    # net_init(tmp_net)\n",
        "\n",
        "    optimizer = optim.Adam(tmp_net.parameters(), lr=0.001)\n",
        "    # optimizer = optim.SGD(tmp_net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    tmp_net = tmp_net.to(device) #モデルを実際に転送\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40, eta_min=1e-5)\n",
        "    loss_fn = LabelSmoothingLoss(classes=10, smoothing=0.1)\n",
        "\n",
        "\n",
        "    train(40, tmp_net, train_loader, validation_loader, optimizer)\n",
        "    val(tmp_net, validation_loader)\n",
        "\n",
        "    models.append(tmp_net)\n",
        "\n",
        "    # ここでモデルの訓練と検証を行います\n",
        "    # 例:\n",
        "    # model = YourModel()\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    # criterion = torch.nn.CrossEntropyLoss()\n",
        "    # for epoch in range(num_epochs):\n",
        "    #     train(...)  # 訓練ループ\n",
        "    #     validate(...)  # 検証ループ\n",
        "\n",
        "    # 訓練と検証の後、必要に応じてモデルの保存やメトリクスの記録を行う\n",
        "    print(f'Training and validation for fold {fold + 1} completed.\\n')\n",
        "\n",
        "\n",
        "# テストデータはここでは使用しませんが、全てのFoldでの訓練が終わった後に使用できます。\n",
        "# 例:\n",
        "# test_model(test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9Fat71rj8zI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "02979577-e8b3-42c8-8699-76a831f6c4d1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cfe4c9d5-d655-48db-8fe4-191c6136320f\", \"submition.csv\", 68906)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "predict_l = []\n",
        "id_l = []\n",
        "id = 0\n",
        "\n",
        "# test_loaderの各バッチでループ\n",
        "for data in test_loader:\n",
        "    with torch.no_grad():\n",
        "        inputs, _ = data\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "        # モデルの予測を格納するテンソルを初期化\n",
        "        final_pred = torch.zeros(inputs.size(0), 10).cuda()  # num_classesはクラス数で置き換えてください\n",
        "\n",
        "        for model in models:\n",
        "            model.eval()\n",
        "            outputs = model(inputs)\n",
        "            final_pred += outputs.data / len(models)  # 平均を取るために各モデルの出力を加算\n",
        "\n",
        "        # 予測ラベルの取得\n",
        "        _, predicted = torch.max(final_pred, 1)\n",
        "        predict_l.extend(predicted.cpu().numpy())  # バッチサイズ分追加\n",
        "        id_l.extend([id + i for i in range(len(predicted))])\n",
        "        id += len(predicted)\n",
        "\n",
        "# DataFrameの作成とCSV出力\n",
        "submit = pd.DataFrame(columns=[\"image_id\", \"labels\"])\n",
        "submit[\"image_id\"] = id_l\n",
        "submit[\"labels\"] = predict_l\n",
        "submit.to_csv(\"submition.csv\", index=False)\n",
        "\n",
        "# Google Colabでファイルをダウンロード\n",
        "from google.colab import files\n",
        "files.download('submition.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqdyfL-DNWXl"
      },
      "source": [
        "# ベースラインはここまで\n",
        "## 精度向上のための今後のToDo\n",
        "### - データ拡張（e.g., Dropout, Flip）について調べて、いろいろなデータ拡張を試してみる\n",
        "### - これまで使用されてきたハイパーパラメーター（バッチサイズなど）の役割を理解して、最適な数値を探索する\n",
        "### - モデルに機能を追加する（e.g., Residual Connectionやnormalizationの追加など）\n",
        "### - 有名な画像認識モデル（ResNetやLeNetなど）について調べて、効果がありそうな機能を追加する\n",
        "### - 信頼できる精度検証手段について調べて実装してみる（e.g., KFold検証）\n",
        "### - エラー解析(どのクラスとどのクラスを間違いやすいか)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diTcQCzevRy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb95fe8-347b-4323-be47-47898258ff6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18751 hours\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/uptime | awk '{print $1 / 60 / 60 \" hours\"}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_loss = 0.0\n",
        "total_acc = 0.0\n",
        "cnt = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in validation_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # 各モデルの予測値を格納\n",
        "        all_outputs = []\n",
        "\n",
        "        for model in models:  # modelsには5つのモデルが格納されている\n",
        "            model.eval()\n",
        "            with autocast(device_type='cuda'):\n",
        "                outputs = model(inputs)\n",
        "            all_outputs.append(outputs)\n",
        "\n",
        "        # 予測値を平均化\n",
        "        avg_outputs = torch.stack(all_outputs).mean(dim=0)\n",
        "        loss = loss_fn(avg_outputs, labels)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(avg_outputs, dim=1)\n",
        "        total_acc += (preds == labels).sum().item() / batch_size_\n",
        "        cnt += 1\n",
        "\n",
        "print(f\"Validation Loss: {total_loss / cnt}, Validation Accuracy: {total_acc / cnt * 100}\")\n"
      ],
      "metadata": {
        "id": "z4yGNA5Io3St",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e575281-f68b-4f5c-9bf9-9cc5498e79ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.5611657792595541, Validation Accuracy: 99.15406050955414\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}